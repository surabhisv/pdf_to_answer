ğŸ“„ PDF to Answer â€“ RAG Chatbot

ğŸ“Œ Overview
PDF to Answer is an intelligent chatbot application built using **LangChain** and **Google Generative AI**, designed to read uploaded PDF files and answer user queries based on their content.  
It implements a **full RAG (Retrieval-Augmented Generation) pipeline** â€” breaking documents into chunks, embedding them into a vector store, retrieving relevant chunks, and generating accurate answers using an LLM.

âœ¨ Features
- ğŸ“‚ **Upload multiple PDF files** at once.
- ğŸ” **Ask natural language questions** about the uploaded content.
- âš¡ **Real-time streaming responses** from the AI model.
- ğŸ”’ Data is stored locally in a **Chroma vector store**.
- ğŸ” **Retry mechanism** for clearing and recreating embeddings database.

ğŸ› ï¸ Tech Stack
- **Frontend & UI:** [Streamlit](https://streamlit.io/)
- **PDF Parsing:** [PyPDF2](https://pypi.org/project/PyPDF2/)
- **LLM & Embeddings:** Google Generative AI (`gemini-pro`), Ollama, Replicate
- **Vector Database:** [Chroma](https://www.trychroma.com/)
- **LangChain Components:**  
  - `RecursiveCharacterTextSplitter` for chunking  
  - `GoogleGenerativeAIEmbeddings` for embeddings  
  - `ChatPromptTemplate` for structured prompting  
  - `StrOutputParser` for clean model output
- **Environment Management:** `dotenv` for API keys

## ğŸš€ How It Works
1. **PDF Upload** â†’ User uploads one or more PDFs.
2. **Text Extraction** â†’ `PyPDF2` extracts raw text from all pages.
3. **Chunking** â†’ Text is split into smaller overlapping chunks using `RecursiveCharacterTextSplitter`.
4. **Embedding Creation** â†’ Chunks are converted to vector embeddings using `GoogleGenerativeAIEmbeddings`.
5. **Storage** â†’ Embeddings are stored locally in a persistent **Chroma** vector store.
6. **Query Processing** â†’ User enters a question; the retriever fetches top-matching chunks.
7. **Answer Generation** â†’ A Large Language Model (`gemini-pro`) generates an accurate and context-aware answer.
